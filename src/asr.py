import whisper
import warnings
import queue
import threading
import sounddevice as sd
import numpy as np
import concurrent.futures
from collections import deque

warnings.filterwarnings("ignore", message="FP16 is not supported on CPU; using FP32 instead")

SAMPLERATE = 16000
CHANNELS = 1
WAKE_WORD = "çš®çš®"
SILENCE_THRESHOLD = 0.01  # RMS é˜ˆå€¼ï¼Œå¯è°ƒ
SILENCE_SECONDS = 2       # é™éŸ³è¶…è¿‡ 2 ç§’åœæ­¢
CHUNK = 1024              # æ¯æ¬¡è¯»å–çš„å¸§æ•°

class ASR:
    def __init__(self, model_size="small"):
        # ä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹åˆ° ~/.cache
        self.model = whisper.load_model(model_size)
        self.audio_queue = queue.Queue()
        self.listening = False
        self.on_wakeup = None

    def audio_callback(self, indata, frames, time, status):
        if status:
            print(status)
        if np.max(np.abs(indata)) < 1e-3:
            return
        self.audio_queue.put(indata.copy())

    def start(self, on_wakeup):
        """ å¯åŠ¨ç›‘å¬ ç›‘æµ‹åˆ°å”¤é†’è¯åè°ƒç”¨å›è°ƒ """
        self.on_wakeup = on_wakeup
        self.listening = True
        threading.Thread(target=self._recognize_stream, daemon=True).start()
        sd.InputStream(callback=self.audio_callback, channels=CHANNELS, samplerate=SAMPLERATE).start()
        print("ğŸ™ï¸ å¼€å§‹ç›‘å¬å”¤é†’è¯...")

    def safe_transcribe(self, float_data):
        with concurrent.futures.ThreadPoolExecutor() as executor:
            future = executor.submit(lambda: self.model.transcribe(
                float_data,
                fp16=False,
                language="zh",
                condition_on_previous_text=False,
                no_speech_threshold=0.5,
                logprob_threshold=-1.0
            ))
            try:
                return future.result(timeout=5)
            except concurrent.futures.TimeoutError:
                print("â±ï¸ Whisper æ¨ç†è¶…æ—¶ï¼Œè·³è¿‡è¯¥ç‰‡æ®µ")
                return {"text": ""}

    def _recognize_stream(self):
        buffer = np.zeros((0, CHANNELS), dtype=np.int16)
        while self.listening:
            try:
                data = self.audio_queue.get()
                buffer = np.concatenate((buffer, data))
                if len(buffer) >= SAMPLERATE:
                    segment = buffer[:SAMPLERATE]
                    buffer = buffer[SAMPLERATE:]
                    float_data = segment.astype(np.float32) / 32768.0
                    if np.max(np.abs(float_data)) < 1e-3 or np.sum(np.abs(float_data)) < 50:
                        continue
                    result = self.safe_transcribe(float_data)
                    print('result', result)
                    text = result.get("text", "").strip()
                    print('text', text)
                    if text:
                        if WAKE_WORD in text:
                            print("âœ… æ£€æµ‹åˆ°å”¤é†’è¯ï¼š", WAKE_WORD)
                            if self.on_wakeup:
                                self.on_wakeup()
            except Exception as e:
                print("è¯†åˆ«é”™è¯¯ï¼š", e)

    def record_phrase(self):
        """ å½•åˆ¶ä¸€æ®µè¯"""
        # è¿™é‡Œä¹Ÿä¸å›ºå®šé•¿åº¦ï¼Œè€Œæ”¹æˆåˆ¤æ–­2ç§’é™éŸ³å°±ç»“æŸ
        print("ğŸ§ å¼€å§‹å½•éŸ³...")
        buffer = []

        # ç”¨ deque ä¿å­˜æœ€è¿‘é™éŸ³çŠ¶æ€ï¼Œç”¨äºæ£€æµ‹è¿ç»­ 2 ç§’é™éŸ³
        silence_window = deque(maxlen=int(SAMPLERATE / CHUNK * SILENCE_SECONDS))

        with sd.InputStream(samplerate=SAMPLERATE, channels=CHANNELS, dtype='int16', blocksize=CHUNK) as stream:
            while True:
                data, _ = stream.read(CHUNK)
                float_data = data.astype(np.float32) / 32768.0
                buffer.append(float_data)

                # è®¡ç®— RMS
                rms = np.sqrt(np.mean(float_data**2))
                silence_window.append(rms < SILENCE_THRESHOLD)

                # å¦‚æœæœ€è¿‘è¿ç»­é™éŸ³è¶…è¿‡ SILENCE_SECONDS
                if len(silence_window) == silence_window.maxlen and all(silence_window):
                    print("â¹ é™éŸ³æ£€æµ‹åˆ°ï¼Œåœæ­¢å½•éŸ³")
                    break

        # æ‹¼æ¥æ‰€æœ‰ç‰‡æ®µ
        audio = np.concatenate(buffer)
        # è½¬å†™
        result = self.model.transcribe(audio, fp16=False, language="zh")
        return result["text"].strip()

    def transcribe_file(self, wav_path: str) -> str:
        result = self.model.transcribe(wav_path)
        return result.get("text", "")

    def transcribe_audio_bytes(self, autio_path: str) -> str:
        return self.transcribe_file(autio_path)